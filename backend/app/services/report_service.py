"""
AI è¡Œä¸šæŠ¥å‘Šç”ŸæˆæœåŠ¡ï¼ˆMarkdown ç‰ˆï¼‰
- åŠ è½½æ¨æ–‡æ•°æ®å¹¶è®¡ç®—åˆ†ææŒ‡æ ‡
- è°ƒç”¨ LLM ç”Ÿæˆè¡Œä¸šæ´å¯Ÿæ‘˜è¦ + é€å¸–åˆ†æ
- æ¸²æŸ“ Markdown æŠ¥å‘Šå¹¶ä¸Šä¼ åˆ° Supabase Storage
"""

import asyncio
import json
import re
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from collections import Counter

from app.services.llm_service import get_llm_service
from app.prompts.report_prompt import (
    REPORT_EXECUTIVE_SUMMARY_PROMPT,
    REPORT_POST_ANALYSIS_PROMPT,
)
from app.services.crawler_service import CRAWL_DATA_BASE_PATH
from app.config import settings

REPORT_OUTPUT_DIR = CRAWL_DATA_BASE_PATH / "reports"
MAX_REPORTS_KEEP = 30

# è‹±æ–‡åœç”¨è¯ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
STOP_WORDS = {
    "the", "a", "an", "is", "are", "was", "were", "be", "been", "being",
    "have", "has", "had", "do", "does", "did", "will", "would", "could",
    "should", "may", "might", "shall", "can", "need", "must", "ought",
    "i", "me", "my", "we", "our", "you", "your", "he", "him", "his",
    "she", "her", "it", "its", "they", "them", "their", "this", "that",
    "these", "those", "am", "in", "on", "at", "to", "for", "of", "with",
    "by", "from", "as", "into", "through", "during", "before", "after",
    "about", "between", "under", "above", "up", "down", "out", "off",
    "over", "again", "further", "then", "once", "here", "there", "when",
    "where", "why", "how", "all", "each", "every", "both", "few", "more",
    "most", "other", "some", "such", "no", "nor", "not", "only", "own",
    "same", "so", "than", "too", "very", "just", "because", "but", "and",
    "or", "if", "while", "what", "which", "who", "whom", "whose", "new",
    "also", "like", "get", "got", "one", "two", "don", "don't", "it's",
    "i'm", "we're", "they're", "he's", "she's", "that's", "there's",
    "what's", "who's", "let's", "here's", "doesn't", "didn't", "won't",
    "can't", "isn't", "aren't", "wasn't", "weren't", "hasn't", "haven't",
    "hadn't", "couldn't", "wouldn't", "shouldn't", "mustn't", "amp",
}


# ==================== å·¥å…·å‡½æ•° ====================

def _parse_twitter_time(ts: str) -> Optional[datetime]:
    """è§£æ Twitter æ—¶é—´æ ¼å¼: 'Wed Sep 27 13:40:54 +0000 2023'"""
    try:
        return datetime.strptime(ts, "%a %b %d %H:%M:%S %z %Y")
    except (ValueError, TypeError):
        return None


def _format_number(n: int) -> str:
    """æ ¼å¼åŒ–æ•°å­—: 1234567 -> 1,234,567"""
    return f"{n:,}"


def _format_short_number(n: int) -> str:
    """ç¼©å†™æ•°å­—: 1500000 -> 1.5M"""
    if n >= 1_000_000:
        return f"{n / 1_000_000:.1f}M"
    if n >= 1_000:
        return f"{n / 1_000:.1f}K"
    return str(n)


def _truncate_text(text: str, max_len: int = 300) -> str:
    """æˆªæ–­æ–‡æœ¬"""
    if len(text) <= max_len:
        return text
    return text[:max_len] + "..."


# ==================== æ•°æ®åŠ è½½ ====================

def load_posts_data(platform: str = "twitter", data_path: str = None) -> Dict[str, Any]:
    """
    åŠ è½½å¸–å­æ•°æ®
    Args:
        platform: å¹³å°ç±»å‹ ("twitter" / "youtube")
        data_path: è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    if data_path:
        filepath = Path(data_path)
    elif platform == "twitter":
        filepath = CRAWL_DATA_BASE_PATH / "twitter" / "posts.json"
    elif platform == "youtube":
        filepath = CRAWL_DATA_BASE_PATH / "youtube" / "videos.json"
    else:
        return {"items": [], "total_count": 0}

    if not filepath.exists():
        return {"items": [], "total_count": 0}

    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)


# ==================== æ•°æ®åˆ†æ ====================

def _engagement_score(item: Dict[str, Any]) -> float:
    """è®¡ç®—å•æ¡æ¨æ–‡çš„äº’åŠ¨åˆ†æ•°"""
    stats = item.get("stats", {})
    return (
        stats.get("likes", 0)
        + stats.get("retweets", 0) * 2
        + stats.get("views", 0) * 0.01
        + stats.get("quotes", 0) * 3
    )


def filter_and_analyze(
    posts_data: Dict[str, Any],
    hours: int = 24,
    authors: Optional[List[str]] = None,
    top_n: int = 10,
) -> Dict[str, Any]:
    """
    æ•°æ®è¿‡æ»¤ä¸åˆ†æ
    Args:
        posts_data: åŸå§‹å¸–å­æ•°æ®
        hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        authors: å¯é€‰çš„ä½œè€…ç”¨æˆ·åè¿‡æ»¤åˆ—è¡¨
        top_n: Top N æ¨æ–‡æ•°é‡
    """
    items = posts_data.get("items", [])
    now = datetime.now(timezone.utc)

    # è§£ææ—¶é—´
    for item in items:
        item["_parsed_time"] = _parse_twitter_time(item.get("created_at", ""))

    # ä½œè€…è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if authors:
        authors_lower = {a.lower().lstrip("@") for a in authors}
        items = [
            item for item in items
            if (item.get("author", {}).get("username") or "").lower() in authors_lower
        ]

    # æ—¶é—´çª—å£è¿‡æ»¤
    recent_items = []
    for item in items:
        pt = item.get("_parsed_time")
        if pt:
            delta = (now - pt).total_seconds() / 3600
            if delta <= hours:
                recent_items.append(item)

    # å¦‚æœæ—¶é—´çª—å£å†…æ¨æ–‡å¤ªå°‘ï¼Œæ”¾å®½åˆ°å…¨éƒ¨æ•°æ®ï¼ˆå‰ 50 æ¡ï¼‰
    use_all = len(recent_items) < 5
    analysis_items = items[:50] if use_all else recent_items

    # Top N é«˜äº’åŠ¨æ¨æ–‡
    sorted_by_engagement = sorted(analysis_items, key=_engagement_score, reverse=True)
    top_posts = sorted_by_engagement[:top_n]

    # æœ€æ´»è·ƒä½œè€…ï¼ˆå¸¦è¯¦ç»†ä¿¡æ¯ï¼‰
    author_post_map: Dict[str, Dict[str, Any]] = {}
    for item in analysis_items:
        author = item.get("author", {})
        username = author.get("username")
        if not username:
            continue
        if username not in author_post_map:
            author_post_map[username] = {
                "username": username,
                "name": author.get("name") or username,
                "avatar": author.get("avatar", ""),
                "followers": author.get("followers", 0),
                "verified": author.get("verified", False),
                "post_count": 0,
            }
        author_post_map[username]["post_count"] += 1
        # æ›´æ–°ä¸ºæœ€æ–°çš„ç²‰ä¸æ•°
        if author.get("followers", 0) > author_post_map[username]["followers"]:
            author_post_map[username]["followers"] = author.get("followers", 0)
            author_post_map[username]["avatar"] = author.get("avatar", "")

    top_authors_by_activity = sorted(
        author_post_map.values(),
        key=lambda x: x["post_count"],
        reverse=True,
    )[:10]

    # å…³é”®è¯é¢‘ç‡
    all_text = " ".join(item.get("text", "") for item in analysis_items)
    all_text = re.sub(r"https?://\S+", "", all_text)
    all_text = re.sub(r"@\w+", "", all_text)
    words = re.findall(r"[a-zA-Z]{3,}", all_text.lower())
    word_freq = Counter(w for w in words if w not in STOP_WORDS)
    top_keywords = word_freq.most_common(20)

    # äº’åŠ¨æ±‡æ€»
    total_likes = sum(item.get("stats", {}).get("likes", 0) for item in analysis_items)
    total_retweets = sum(item.get("stats", {}).get("retweets", 0) for item in analysis_items)
    total_views = sum(item.get("stats", {}).get("views", 0) for item in analysis_items)

    active_authors = len(set(
        item.get("author", {}).get("username", "")
        for item in analysis_items
        if item.get("author", {}).get("username")
    ))

    # æ¸…ç†ä¸´æ—¶å­—æ®µ
    for item in posts_data.get("items", []):
        item.pop("_parsed_time", None)

    return {
        "total_posts": posts_data.get("total_count", len(posts_data.get("items", []))),
        "analysis_posts_count": len(analysis_items),
        "used_full_data": use_all,
        "hours": hours,
        "top_n": top_n,
        "top_posts": top_posts,
        "top_authors_by_activity": top_authors_by_activity,
        "top_keywords": top_keywords,
        "total_likes": total_likes,
        "total_retweets": total_retweets,
        "total_views": total_views,
        "active_authors": active_authors,
        "scraped_at": posts_data.get("scraped_at", ""),
    }


# ==================== LLM æ´å¯Ÿç”Ÿæˆ ====================

def build_llm_input(top_posts: List[Dict[str, Any]]) -> str:
    """
    æ„å»º LLM è¾“å…¥æ–‡æœ¬ï¼ŒåªåŒ…å« Top N æ¨æ–‡çš„ç²¾ç®€ä¿¡æ¯
    æ§åˆ¶æ€»è¾“å…¥åœ¨ 2000-3000 å­—ä»¥å†…
    """
    lines = []
    for i, post in enumerate(top_posts, 1):
        author = post.get("author", {})
        stats = post.get("stats", {})
        name = author.get("name") or author.get("username") or "Unknown"
        username = author.get("username", "")
        text = _truncate_text(post.get("text", ""), 300)

        line = (
            f"[{i}] @{username} ({name}, {_format_short_number(author.get('followers', 0))} followers)\n"
            f"å†…å®¹: {text}\n"
            f"äº’åŠ¨: {_format_short_number(stats.get('likes', 0))} likes, "
            f"{_format_short_number(stats.get('retweets', 0))} RT, "
            f"{_format_short_number(stats.get('views', 0))} views"
        )

        # å¼•ç”¨æ¨æ–‡
        qt = post.get("quoted_tweet")
        if qt:
            qt_author = qt.get("author", {})
            qt_text = _truncate_text(qt.get("text", ""), 200)
            line += (
                f"\nå¼•ç”¨æ¨æ–‡ @{qt_author.get('username', '')}: {qt_text}"
            )

        lines.append(line)

    return "\n\n".join(lines)


async def _call_llm(
    message: str,
    system_prompt: str,
    max_tokens: int,
    llm_config: Optional[Dict[str, Any]] = None,
) -> str:
    """é€šç”¨ LLM è°ƒç”¨å°è£…"""
    config = llm_config or {}
    api_key = config.get("api_key")
    model = config.get("model")

    llm = get_llm_service(api_key=api_key, model=model)

    # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰ base_urlï¼Œéœ€è¦é‡æ–°åˆ›å»º llm å®ä¾‹
    if config.get("base_url"):
        from langchain_openai import ChatOpenAI
        try:
            from langchain_core.messages import HumanMessage, SystemMessage
        except ImportError:
            from langchain.schema import HumanMessage, SystemMessage

        custom_llm = ChatOpenAI(
            model=model or settings.LLM_MODEL,
            openai_api_key=api_key or settings.SILICONFLOW_API_KEY,
            openai_api_base=config["base_url"],
            temperature=0.4,
            max_tokens=max_tokens,
            request_timeout=settings.LLM_REQUEST_TIMEOUT,
        )
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=message),
        ]
        response = custom_llm.invoke(messages)
        return response.content.strip()

    return llm.chat(
        message=message,
        system_prompt=system_prompt,
        temperature=0.4,
        max_tokens=max_tokens,
    ).strip()


async def generate_executive_summary(
    top_posts: List[Dict[str, Any]],
    llm_config: Optional[Dict[str, Any]] = None,
) -> str:
    """
    ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆæ€§è¡Œä¸šæ´å¯Ÿæ‘˜è¦ï¼ˆ300-500å­—ï¼‰
    """
    llm_input = build_llm_input(top_posts)

    try:
        return await _call_llm(
            message=llm_input,
            system_prompt=REPORT_EXECUTIVE_SUMMARY_PROMPT,
            max_tokens=1000,
            llm_config=llm_config,
        )
    except Exception as e:
        print(f"[Report] Executive summary generation failed: {e}")
        return _fallback_executive_summary(top_posts)


async def generate_post_analyses(
    top_posts: List[Dict[str, Any]],
    llm_config: Optional[Dict[str, Any]] = None,
) -> Dict[int, str]:
    """
    ä½¿ç”¨ LLM ç”Ÿæˆé€å¸–åˆ†æã€‚è¿”å› {å¸–å­åºå·: åˆ†ææ–‡æœ¬}
    """
    llm_input = build_llm_input(top_posts)

    try:
        raw_output = await _call_llm(
            message=llm_input,
            system_prompt=REPORT_POST_ANALYSIS_PROMPT,
            max_tokens=2500,
            llm_config=llm_config,
        )
        return _parse_post_analyses(raw_output, len(top_posts))
    except Exception as e:
        print(f"[Report] Post analyses generation failed: {e}")
        return {}


def _parse_post_analyses(raw_output: str, num_posts: int) -> Dict[int, str]:
    """
    è§£æ LLM è¾“å‡ºçš„é€å¸–åˆ†æ
    æ ¼å¼: [1] åˆ†æå†…å®¹...\n\n[2] åˆ†æå†…å®¹...\n\n
    """
    result = {}

    # å°è¯•æŒ‰ [N] æ ‡è®°åˆ†å‰²
    parts = re.split(r'\[(\d+)\]\s*', raw_output)
    # parts æ ¼å¼: ['', '1', 'åˆ†æå†…å®¹', '2', 'åˆ†æå†…å®¹', ...]
    if len(parts) >= 3:
        for i in range(1, len(parts) - 1, 2):
            try:
                idx = int(parts[i])
                text = parts[i + 1].strip()
                if text and 1 <= idx <= num_posts:
                    result[idx] = text
            except (ValueError, IndexError):
                continue

    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰åŒæ¢è¡Œåˆ†å‰²å¹¶é¡ºåºæ˜ å°„
    if not result:
        paragraphs = [p.strip() for p in raw_output.split("\n\n") if p.strip()]
        for i, para in enumerate(paragraphs[:num_posts], 1):
            # å»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
            cleaned = re.sub(r'^\d+[.ã€)\]]\s*', '', para)
            if cleaned:
                result[i] = cleaned

    return result


def _fallback_executive_summary(top_posts: List[Dict[str, Any]]) -> str:
    """LLM å¤±è´¥æ—¶çš„é™çº§æ‘˜è¦å†…å®¹"""
    if not top_posts:
        return "æš‚æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆæ´å¯Ÿã€‚"

    top = top_posts[0]
    author = top.get("author", {})
    stats = top.get("stats", {})
    name = author.get("name") or author.get("username") or "Unknown"

    return (
        f"æœ¬æ¬¡åˆ†æå…±ç­›é€‰å‡º {len(top_posts)} æ¡é«˜äº’åŠ¨æ¨æ–‡ã€‚"
        f"å…¶ä¸­æ¥è‡ª {name} çš„æ¨æ–‡è·å¾—äº†æœ€é«˜äº’åŠ¨"
        f"ï¼ˆ{_format_short_number(stats.get('views', 0))} æµè§ˆï¼Œ"
        f"{_format_short_number(stats.get('likes', 0))} ç‚¹èµï¼‰ã€‚"
    )


# ==================== Markdown æŠ¥å‘Šæ¸²æŸ“ ====================

def render_markdown_report(
    analytics: Dict[str, Any],
    executive_summary: str,
    post_analyses: Dict[int, str],
    report_time: datetime = None,
) -> str:
    """
    å°†åˆ†ææ•°æ® + LLM æ´å¯Ÿç»„è£…ä¸º Markdown æ ¼å¼æŠ¥å‘Šï¼ˆå¯åµŒå…¥HTML divæ ‡ç­¾ä¼˜åŒ–æ’ç‰ˆï¼‰
    """
    if report_time is None:
        report_time = datetime.now()

    date_str = report_time.strftime("%Y-%m-%d")
    time_str = report_time.strftime("%Y-%m-%d %H:%M")

    # æ—¶é—´çª—å£æè¿°
    hours = analytics["hours"]
    time_window = f"æœ€è¿‘ {hours} å°æ—¶" if not analytics["used_full_data"] else "å…¨éƒ¨æ•°æ®ï¼ˆæœ€è¿‘50æ¡ï¼‰"

    # æ„å»ºæŠ¥å‘Šå†…å®¹
    md_lines = []

    # æ ‡é¢˜å’Œå…ƒæ•°æ®
    md_lines.append(f"# AI è¡Œä¸šæ¨ç‰¹æ—¥æŠ¥ | {date_str}\n")
    md_lines.append(f"> æ•°æ®çª—å£ï¼š{time_window} | åˆ†ææ¨æ–‡ï¼š{analytics['analysis_posts_count']} æ¡ | æ´»è·ƒä½œè€…ï¼š{analytics['active_authors']} ä½ | æ€»æµè§ˆé‡ï¼š{_format_short_number(analytics['total_views'])}\n")
    md_lines.append("---\n")

    # ç»¼åˆæ´å¯Ÿéƒ¨åˆ†
    md_lines.append("## æ ¸å¿ƒæ´å¯Ÿ\n")
    # å°†LLMç”Ÿæˆçš„æ´å¯ŸæŒ‰æ®µè½åˆ†å‰²
    insights_paragraphs = [p.strip() for p in executive_summary.split("\n\n") if p.strip()]
    for para in insights_paragraphs:
        # æå–æ®µè½æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        lines = para.split("\n")
        if len(lines) > 1 and len(lines[0]) < 50:
            md_lines.append(f"### {lines[0]}")
            md_lines.append("\n".join(lines[1:]) + "\n")
        else:
            md_lines.append(para + "\n")
    md_lines.append("---\n")

    # Topæ¨æ–‡éƒ¨åˆ†
    md_lines.append(f"## é«˜äº’åŠ¨æ¨æ–‡ Top {len(analytics['top_posts'])}\n")

    for i, post in enumerate(analytics['top_posts'], 1):
        author = post.get("author", {})
        stats = post.get("stats", {})
        name = author.get("name") or author.get("username") or "Unknown"
        username = author.get("username", "")
        avatar = author.get("avatar", "")
        followers = _format_short_number(author.get("followers", 0))
        verified = " âœ“" if author.get("verified") else ""
        text = post.get("text", "")
        url = post.get("url", "")

        md_lines.append(f"### {i}. {name}\n")

        # ä½œè€…ä¿¡æ¯ï¼ˆåµŒå…¥HTMLä¼˜åŒ–å¤´åƒæ˜¾ç¤ºï¼‰
        if avatar:
            md_lines.append(f'<img src="{avatar}" width="24" height="24" style="border-radius:50%;vertical-align:middle;"> **{name}** Â· @{username} Â· {followers} followers{verified}\n')
        else:
            md_lines.append(f"**{name}** Â· @{username} Â· {followers} followers{verified}\n")

        # æ¨æ–‡å†…å®¹
        md_lines.append(f"> {text}\n")

        # å¼•ç”¨æ¨æ–‡
        qt = post.get("quoted_tweet")
        if qt:
            qt_author = qt.get("author", {})
            qt_name = qt_author.get("name") or qt_author.get("username") or "Unknown"
            qt_username = qt_author.get("username", "")
            qt_text = qt.get("text", "")
            qt_url = qt.get("url", "")

            qt_link = f": [åŸæ–‡]({qt_url})" if qt_url else ""
            md_lines.append(f"> **å¼•ç”¨ @{qt_username}** ({qt_name}){qt_link}")
            md_lines.append(f"> ")
            for line in qt_text.split("\n"):
                md_lines.append(f"> {line}")
            md_lines.append("")

        # LLMåˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
        if i in post_analyses:
            analysis_text = post_analyses[i]
            md_lines.append('<div style="background: linear-gradient(135deg, #f0fdf4 0%, #ecfdf5 100%); border-left: 4px solid #10b981; border-radius: 8px; padding: 12px 16px; margin: 12px 0;">')
            md_lines.append(f'<strong>ğŸ” AIåˆ†æï¼š</strong> {analysis_text}')
            md_lines.append('</div>\n')

        # äº’åŠ¨æ•°æ®
        likes = _format_short_number(stats.get("likes", 0))
        retweets = _format_short_number(stats.get("retweets", 0))
        views = _format_short_number(stats.get("views", 0))
        replies = _format_short_number(stats.get("replies", 0))

        view_link = f" Â· [æŸ¥çœ‹åŸæ–‡]({url})" if url else ""
        md_lines.append(f"likes {likes} Â· RT {retweets} Â· views {views} Â· replies {replies}{view_link}\n")

    md_lines.append("\n---\n")
    md_lines.append(f"\n*ç”± Athena è‡ªåŠ¨ç”Ÿæˆ | {time_str}*\n")

    return "\n".join(md_lines)


# ==================== å­˜å‚¨ ====================

def save_markdown_report(md_content: str, filename: str = None) -> Tuple[str, str]:
    """
    ä¿å­˜ Markdown æŠ¥å‘Šåˆ°æœ¬åœ°ç£ç›˜ï¼Œæ¸…ç†æ—§æŠ¥å‘Šã€‚
    è¿”å› (ç»å¯¹æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å)
    """
    REPORT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    if not filename:
        filename = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

    filepath = REPORT_OUTPUT_DIR / filename
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)

    # æ¸…ç†æ—§æŠ¥å‘Š
    reports = sorted(
        list(REPORT_OUTPUT_DIR.glob("report_*.md")),
        key=lambda p: p.stat().st_mtime,
        reverse=True,
    )
    for old in reports[MAX_REPORTS_KEEP:]:
        old.unlink(missing_ok=True)

    return str(filepath), filename


def get_report_url(filename: str, base_url: str = None) -> str:
    """æ„å»ºæŠ¥å‘Šçš„æœ¬åœ°å¯è®¿é—® URLï¼ˆå¤‡ç”¨ï¼‰"""
    if not base_url:
        base_url = "http://localhost:8000"
    base_url = base_url.rstrip("/")
    return f"{base_url}/reports/{filename}"


async def upload_to_supabase(
    pdf_bytes: bytes,
    filename: str,
    storage_config: Optional[Dict[str, Any]] = None,
) -> Optional[str]:
    """
    ä¸Šä¼ æŠ¥å‘Šæ–‡ä»¶åˆ° Supabase Storageï¼ˆä½¿ç”¨ REST API ç›´æ¥ä¸Šä¼ ï¼‰
    Args:
        pdf_bytes: æ–‡ä»¶å­—èŠ‚ï¼ˆå¯ä»¥æ˜¯PDFæˆ–MDï¼‰
        filename: æ–‡ä»¶å
        storage_config: å¯é€‰çš„è‡ªå®šä¹‰å­˜å‚¨é…ç½® {supabase_url, supabase_key, bucket}
    Returns:
        å…¬å¼€è®¿é—® URLï¼Œå¤±è´¥è¿”å› None
    """
    config = storage_config or {}
    supabase_url = config.get("supabase_url") or settings.SUPABASE_URL
    supabase_key = config.get("supabase_key") or settings.SUPABASE_SECRET_KEY
    bucket = config.get("bucket", "reports")

    if not supabase_url or not supabase_key:
        print("[Report] Supabase not configured, skipping upload")
        return None

    try:
        import httpx

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åå†³å®šä¸Šä¼ è·¯å¾„
        if filename.endswith('.md'):
            file_path = f"markdown/{filename}"
            content_type = "text/markdown"
        elif filename.endswith('.pdf'):
            file_path = f"pdf/{filename}"
            content_type = "application/pdf"
        else:
            file_path = filename
            content_type = "application/octet-stream"

        # Supabase Storage REST API endpoints
        storage_base_url = f"{supabase_url}/storage/v1"
        upload_url = f"{storage_base_url}/object/{bucket}/{file_path}"

        headers = {
            "apikey": supabase_key,
            "Authorization": f"Bearer {supabase_key}",
            "Content-Type": content_type,
        }

        # ä¸Šä¼ æ–‡ä»¶ï¼ˆä½¿ç”¨ x-upsert è¦†ç›–åŒåæ–‡ä»¶ï¼‰
        upload_headers = {**headers, "x-upsert": "true"}
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                upload_url,
                content=pdf_bytes,
                headers=upload_headers,
            )
            response.raise_for_status()

        # ç”Ÿæˆå…¬å¼€è®¿é—® URL
        public_url = f"{storage_base_url}/object/public/{bucket}/{file_path}"
        print(f"[Report] Uploaded to Supabase: {public_url}")
        return public_url

    except httpx.HTTPStatusError as e:
        print(f"[Report] Supabase upload failed (HTTP {e.response.status_code}): {e.response.text}")
        return None
    except Exception as e:
        print(f"[Report] Supabase upload failed: {e}")
        return None


# ==================== å®Œæ•´æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿ ====================

async def generate_report(
    platform: str = "twitter",
    hours: int = 24,
    authors: Optional[List[str]] = None,
    top_n: int = 10,
    report_style: str = "daily_insight",
    storage_config: Optional[Dict[str, Any]] = None,
    llm_config: Optional[Dict[str, Any]] = None,
    data_path: str = None,
) -> Dict[str, Any]:
    """
    å®Œæ•´æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿
    Args:
        platform: å¹³å° ("twitter" / "youtube")
        hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        authors: å¯é€‰çš„ä½œè€…è¿‡æ»¤åˆ—è¡¨
        top_n: Top N æ¨æ–‡æ•°é‡
        report_style: æŠ¥å‘Šé£æ ¼ï¼ˆé¢„ç•™ï¼‰
        storage_config: å¯é€‰çš„ Supabase å­˜å‚¨é…ç½®
        llm_config: å¯é€‰çš„ LLM é…ç½®
        data_path: å¯é€‰çš„è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„
    Returns:
        {report_url, insights, analytics_summary, filename, generated_at, format}
    """
    # 1. åŠ è½½æ•°æ®
    posts_data = load_posts_data(platform=platform, data_path=data_path)
    if not posts_data.get("items"):
        return {
            "report_url": None,
            "insights": None,
            "text_summary": "å½“å‰æ²¡æœ‰å¯ç”¨çš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«ã€‚",
            "filename": None,
            "generated_at": datetime.now().isoformat(),
            "analytics_summary": None,
            "format": "markdown",
        }

    # 2. æ•°æ®è¿‡æ»¤ä¸åˆ†æ
    analytics = filter_and_analyze(
        posts_data,
        hours=hours,
        authors=authors,
        top_n=top_n,
    )

    report_time = datetime.now()

    # 3. LLM ç”Ÿæˆæ´å¯Ÿï¼ˆä¸¤ä¸ªå¹¶å‘è°ƒç”¨ï¼‰
    executive_summary, post_analyses = await asyncio.gather(
        generate_executive_summary(analytics["top_posts"], llm_config=llm_config),
        generate_post_analyses(analytics["top_posts"], llm_config=llm_config),
    )

    # 4. æ¸²æŸ“ Markdown æŠ¥å‘Š
    md_report = render_markdown_report(
        analytics, executive_summary, post_analyses, report_time=report_time,
    )

    # 5. ä¿å­˜ Markdown åˆ°æœ¬åœ°
    md_filename = f"report_{report_time.strftime('%Y%m%d_%H%M%S')}.md"
    local_md_path, md_filename = save_markdown_report(md_report, filename=md_filename)

    # 6. ä¸Šä¼ åˆ° Supabase Storage
    md_bytes = md_report.encode('utf-8')
    report_url = await upload_to_supabase(md_bytes, md_filename, storage_config=storage_config)

    # å¦‚æœ Supabase ä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ° URL ä½œä¸ºå¤‡ç”¨
    if not report_url:
        report_url = get_report_url(md_filename)

    return {
        "report_url": report_url,
        "insights": executive_summary,
        "text_summary": executive_summary,  # å…¼å®¹æ—§æ¥å£
        "filename": md_filename,
        "generated_at": report_time.isoformat(),
        "format": "markdown",
        "analytics_summary": {
            "total_posts": analytics["total_posts"],
            "analysis_posts_count": analytics["analysis_posts_count"],
            "total_views": analytics["total_views"],
            "total_likes": analytics["total_likes"],
            "total_retweets": analytics["total_retweets"],
            "active_authors": analytics["active_authors"],
            "top_keywords": [word for word, _ in analytics["top_keywords"][:10]],
        },
    }


# ==================== å…¼å®¹æ—§æ¥å£ ====================

async def generate_full_report(
    hours: int = 24,
    base_url: str = None,
    api_key: str = None,
) -> Dict[str, Any]:
    """
    å…¼å®¹æ—§æ¥å£çš„æŠ¥å‘Šç”Ÿæˆå‡½æ•°
    å†…éƒ¨è°ƒç”¨æ–°çš„ generate_report()
    """
    llm_config = {"api_key": api_key} if api_key else None
    result = await generate_report(
        platform="twitter",
        hours=hours,
        llm_config=llm_config,
    )

    # æ˜ å°„å›æ—§æ ¼å¼
    return {
        "html_path": None,
        "report_url": result.get("report_url"),
        "text_summary": result.get("text_summary", ""),
        "filename": result.get("filename"),
        "generated_at": result.get("generated_at"),
        "analytics_summary": result.get("analytics_summary"),
    }
